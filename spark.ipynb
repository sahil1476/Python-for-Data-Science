{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data cframe into spoark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "// Create SparkSession\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"SparkCSVExample\")\n",
    "  .getOrCreate()\n",
    "\n",
    "// Load CSV file into DataFrame\n",
    "val df = spark.read\n",
    "  .option(\"header\", \"true\")\n",
    "  .csv(\"path/to/T1.csv\")\n",
    "\n",
    "// Show the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "// Display first few rows of the DataFrame\n",
    "df.show()\n",
    "\n",
    "// Perform any operations you need on the DataFrame, such as filtering, aggregations, etc.\n",
    "// For example, to filter rows where Wind Speed is greater than 10:\n",
    "val filteredDF = df.filter($\"Wind Speed\" > 10)\n",
    "filteredDF.show()\n",
    "\n",
    "// Perform other transformations or analyses as required\n",
    "// Remember to handle any necessary type conversions or data cleaning as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### publish the data from your CSV file into Apache Kafka in a streaming fashion using Apache Spark, \n",
    "##### you can use the kafka sink provided by Spark Structured Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_json, struct\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSVToKafka\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the CSV file\n",
    "schema = StructType().add(\"Date/Time\", \"string\") \\\n",
    "                     .add(\"LV ActivePower\", \"double\") \\\n",
    "                     .add(\"Wind Speed\", \"double\") \\\n",
    "                     .add(\"Theo_Power_Curve\", \"double\") \\\n",
    "                     .add(\"Wind Direction\", \"double\")\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = \"path/to/T1.csv\"\n",
    "\n",
    "# Read CSV file into DataFrame\n",
    "df = spark.readStream \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(csv_file_path)\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"your_topic_name\"\n",
    "\n",
    "# Define the Kafka sink\n",
    "kafka_sink = df.selectExpr(\"to_json(struct(*)) AS value\") \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"topic\", kafka_topic) \\\n",
    "    .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\\n",
    "    .start()\n",
    "\n",
    "# Start the Kafka sink\n",
    "kafka_sink.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read data from Kafka in a streaming fashion using PySpark, you can use the readStream \n",
    "#### method along with the Kafka source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToSpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"your_topic_name\"\n",
    "\n",
    "# Define the Kafka source\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Print the schema of the streaming DataFrame\n",
    "df.printSchema()\n",
    "'''\n",
    "# Convert value column from binary to string\n",
    "df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "'''\n",
    "\n",
    "\n",
    "# Start the streaming query\n",
    "query = df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the termination of the streaming query\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write the data received from Kafka into a Delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note how to make delta table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreateDeltaTable\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [(1, 'Alice'), (2, 'Bob'), (3, 'Charlie')]\n",
    "columns = ['id', 'name']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define the path for the Delta table\n",
    "delta_table_path = \"path/to/delta_table\"\n",
    "\n",
    "# Write the DataFrame as a Delta table\n",
    "df.write.format(\"delta\").save(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, to_date, current_date, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, TimestampType, MapType\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToDelta\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"your_topic_name\"\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Date/Time\", StringType(), True),\n",
    "    StructField(\"LV ActivePower\", StringType(), True),\n",
    "    StructField(\"Wind Speed\", StringType(), True),\n",
    "    StructField(\"Theo_Power_Curve\", StringType(), True),\n",
    "    StructField(\"Wind Direction\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define Kafka source\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert value column from binary to string and apply schema\n",
    "df = df.selectExpr(\"CAST(value AS STRING) as value\") \\\n",
    "    .select(from_json(\"value\", schema).alias(\"data\")) \\\n",
    "    .selectExpr(\"data.*\")\n",
    "\n",
    "# Convert 'Date/Time' column to DateType\n",
    "df = df.withColumn(\"signal_date\", to_date(\"Date/Time\", \"yyyy-MM-dd\"))\n",
    "\n",
    "# Define signals map\n",
    "signals_map = {\n",
    "    \"LV ActivePower\": \"LV ActivePower\",\n",
    "    \"Wind Speed\": \"Wind Speed\",\n",
    "    \"Theo_Power_Curve\": \"Theo_Power_Curve\",\n",
    "    \"Wind Direction\": \"Wind Direction\"\n",
    "}\n",
    "\n",
    "# Create signals map column\n",
    "df = df.withColumn(\"signals\", struct([df[col].alias(col) for col in signals_map]))\n",
    "\n",
    "# Define Delta table schema\n",
    "delta_schema = StructType([\n",
    "    StructField(\"signal_date\", DateType(), True),\n",
    "    StructField(\"signal_tc\", TimestampType(), True),\n",
    "    StructField(\"create_date\", DateType(), True),\n",
    "    StructField(\"create_ts\", TimestampType(), True),\n",
    "    StructField(\"signals\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "# Write the DataFrame to Delta table\n",
    "query = df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\\n",
    "    .start(\"path/to/delta_table\")\n",
    "\n",
    "# Wait for the termination of the streaming query\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## alternate for the above schema\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, to_date, to_timestamp, current_date, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, TimestampType, MapType\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToDelta\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"your_topic_name\"\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"Date/Time\", StringType(), True),\n",
    "    StructField(\"LV ActivePower\", StringType(), True),\n",
    "    StructField(\"Wind Speed\", StringType(), True),\n",
    "    StructField(\"Theo_Power_Curve\", StringType(), True),\n",
    "    StructField(\"Wind Direction\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define Kafka source\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert value column from binary to string and apply schema\n",
    "df = df.selectExpr(\"CAST(value AS STRING) as value\") \\\n",
    "    .select(from_json(\"value\", schema).alias(\"data\")) \\\n",
    "    .selectExpr(\"data.*\")\n",
    "\n",
    "# Convert 'Date/Time' column to DateType and signal_tc to TimestampType\n",
    "df = df.withColumn(\"signal_date\", to_date(\"Date/Time\", \"dd MM yyyy HH:mm\")) \\\n",
    "    .withColumn(\"signal_tc\", to_timestamp(\"Date/Time\", \"dd MM yyyy HH:mm\"))\n",
    "\n",
    "# Define signals map\n",
    "signals_map = {\n",
    "    \"LV ActivePower\": \"LV ActivePower\",\n",
    "    \"Wind Speed\": \"Wind Speed\",\n",
    "    \"Theo_Power_Curve\": \"Theo_Power_Curve\",\n",
    "    \"Wind Direction\": \"Wind Direction\"\n",
    "}\n",
    "\n",
    "# Create signals map column\n",
    "df = df.withColumn(\"signals\", struct([df[col].alias(col) for col in signals_map]))\n",
    "\n",
    "# Define Delta table schema\n",
    "delta_schema = StructType([\n",
    "    StructField(\"signal_date\", DateType(), True),\n",
    "    StructField(\"signal_tc\", TimestampType(), True),\n",
    "    StructField(\"create_date\", DateType(), True),\n",
    "    StructField(\"create_ts\", TimestampType(), True),\n",
    "    StructField(\"signals\", MapType(StringType(), StringType()), True)\n",
    "])\n",
    "\n",
    "# Write the DataFrame to Delta table\n",
    "query = df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\\n",
    "    .start(\"path/to/delta_table\")\n",
    "\n",
    "# Wait for the termination of the streaming query\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### read data from a Delta Lake table using Spark, you can use the ' read ' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadDeltaLake\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the Delta table\n",
    "delta_table_path = \"path/to/delta_table\"\n",
    "\n",
    "# Read Delta table as DataFrame\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n",
    "\n",
    "# Perform any further operations on the DataFrame as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To calculate the distinct 'signal_tc' datapoints per day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DistinctSignalTcPerDay\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the Delta table\n",
    "delta_table_path = \"path/to/delta_table\"\n",
    "\n",
    "# Read Delta table as DataFrame\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Extract date from 'signal_tc'\n",
    "df = df.withColumn(\"date\", date_format(col(\"signal_tc\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Calculate distinct 'signal_tc' datapoints per day\n",
    "distinct_signal_tc_per_day = df.groupBy(\"date\").agg({\"signal_tc\": \"count\"}).orderBy(\"date\")\n",
    "\n",
    "# Show the results\n",
    "distinct_signal_tc_per_day.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average value of all the signals per hour,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, hour\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AverageSignalPerHour\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the Delta table\n",
    "delta_table_path = \"path/to/delta_table\"\n",
    "\n",
    "# Read Delta table as DataFrame\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Extract hour from 'signal_tc'\n",
    "df = df.withColumn(\"hour\", hour(col(\"signal_tc\")))\n",
    "\n",
    "# Group by hour and calculate average for each signal\n",
    "average_per_hour = df.groupBy(\"hour\").agg(\n",
    "    {\"LV ActivePower\": \"avg\", \n",
    "     \"Wind Speed\": \"avg\", \n",
    "     \"Theo_Power_Curve\": \"avg\", \n",
    "     \"Wind Direction\": \"avg\"}\n",
    ").orderBy(\"hour\")\n",
    "\n",
    "# Show the results\n",
    "average_per_hour.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

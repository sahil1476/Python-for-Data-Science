{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple write into kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PublishToKafka\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = \"path/to/T1.csv\"\n",
    "\n",
    "# Read CSV file into DataFrame\n",
    "df = spark.read.option(\"header\", \"true\").csv(csv_file_path)\n",
    "\n",
    "# Convert DataFrame to JSON and select necessary columns\n",
    "json_df = df.selectExpr(\"to_json(struct(*)) AS value\")\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"your_topic_name\"\n",
    "\n",
    "# Write data to Kafka\n",
    "query = json_df.write \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"topic\", kafka_topic) \\\n",
    "    .save()\n",
    "\n",
    "# Wait for the query to terminate\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadCSV\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = \"path/to/T1.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "publish into kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_json, struct\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PublishToKafka\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = \"path/to/T1.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_file_path, header=True)\n",
    "\n",
    "# Convert the DataFrame to JSON\n",
    "json_df = df.select(to_json(struct(\"*\")).alias(\"value\"))\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"your_topic_name\"\n",
    "\n",
    "# Write the JSON data to Kafka\n",
    "query = json_df.writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"topic\", kafka_topic) \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the query to terminate\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read from kafka back to spaark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadFromKafka\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"your_topic_name\"\n",
    "\n",
    "# Read data from Kafka into a DataFrame\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .load()\n",
    "\n",
    "# Start the streaming query\n",
    "query = df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the query to terminate\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the DataFrame is streaming\n",
    "if df.isStreaming:\n",
    "    print(\"DataFrame is streaming. Monitoring for new data...\")\n",
    "else:\n",
    "    print(\"DataFrame is not streaming. No new data to monitor.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write to delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, to_date, current_date, current_timestamp, struct\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WriteToDelta\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"your_topic_name\"\n",
    "\n",
    "# Read data from Kafka into a DataFrame\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .load()\n",
    "\n",
    "# Convert value column from binary to string and parse JSON\n",
    "df = df.selectExpr(\"CAST(value AS STRING) as value\") \\\n",
    "    .select(from_json(\"value\", \"signal_date DATE, signal_tc TIMESTAMP, signals MAP<STRING, STRING>\").alias(\"data\")) \\\n",
    "    .select(\"data.signal_date\", \"data.signal_tc\", \"data.signals\")\n",
    "\n",
    "# Add create_date and create_ts columns\n",
    "df = df.withColumn(\"create_date\", current_date()) \\\n",
    "    .withColumn(\"create_ts\", current_timestamp())\n",
    "\n",
    "# Write the DataFrame to Delta table\n",
    "delta_table_path = \"path/to/delta_table\"\n",
    "query = df.writeStream \\  # in lace of writeStream check with write\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\\n",
    "    .start(delta_table_path)\n",
    "\n",
    "# Wait for the query to terminate\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if delta table is not created in the table\n",
    "# including required package\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession with Delta Lake package\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"YourAppName\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:1.0.0\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read delta lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ReadDeltaLake\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the Delta table\n",
    "delta_table_path = \"path/to/delta_table\"\n",
    "\n",
    "# Read Delta table as DataFrame\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distinct datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, countDistinct\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DistinctSignalTcPerDay\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the Delta table\n",
    "delta_table_path = \"path/to/delta_table\"\n",
    "\n",
    "# Read Delta table as DataFrame\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Extract date and hour from 'signal_tc'\n",
    "df = df.withColumn(\"date\", date_format(col(\"signal_tc\"), \"yyyy-MM-dd\"))\n",
    "df = df.withColumn(\"hour\", date_format(col(\"signal_tc\"), \"H\"))\n",
    "\n",
    "# Calculate distinct 'signal_tc' datapoints per hour\n",
    "distinct_signal_tc_per_hour = df.groupBy(\"date\", \"hour\").agg(countDistinct(\"signal_tc\").alias(\"distinct_signal_tc_count\"))\n",
    "\n",
    "# Calculate total distinct 'signal_tc' datapoints per day\n",
    "distinct_signal_tc_per_day = distinct_signal_tc_per_hour.groupBy(\"date\").sum(\"distinct_signal_tc_count\")\n",
    "\n",
    "# Show the results\n",
    "distinct_signal_tc_per_day.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import hour, avg\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AverageSignalPerHour\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the Delta table\n",
    "delta_table_path = \"path/to/delta_table\"\n",
    "\n",
    "# Read Delta table as DataFrame\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Extract hour from 'signal_tc'\n",
    "df = df.withColumn(\"hour\", hour(\"signal_tc\"))\n",
    "\n",
    "# Group by hour and calculate average for each signal\n",
    "average_per_hour = df.groupBy(\"hour\").agg(\n",
    "    avg(\"LV ActivePower\").alias(\"avg_LV_ActivePower\"),\n",
    "    avg(\"Wind Speed\").alias(\"avg_Wind_Speed\"),\n",
    "    avg(\"Theo_Power_Curve\").alias(\"avg_Theo_Power_Curve\"),\n",
    "    avg(\"Wind Direction\").alias(\"avg_Wind_Direction\")\n",
    ")\n",
    "\n",
    "# Show the results\n",
    "average_per_hour.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import hour, col\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AverageSignalPerHour\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the Delta table\n",
    "delta_table_path = \"path/to/delta_table\"\n",
    "\n",
    "# Read Delta table as DataFrame\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Extract hour from 'signal_tc'\n",
    "df = df.withColumn(\"hour\", hour(\"signal_tc\"))\n",
    "\n",
    "# Group by hour and calculate the average of all signals combined\n",
    "average_all_signals_per_hour = df.groupBy(\"hour\").agg(\n",
    "    (col(\"LV ActivePower\") + col(\"Wind Speed\") + col(\"Theo_Power_Curve\") + col(\"Wind Direction\")) / 4.0\n",
    "    ).alias(\"average_all_signals_per_hour\")\n",
    "\n",
    "# Show the results\n",
    "average_all_signals_per_hour.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AddGenerationIndicator\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Path to the Delta table\n",
    "delta_table_path = \"path/to/delta_table\"\n",
    "\n",
    "# Read Delta table as DataFrame\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Add 'generation_indicator' column based on 'LV ActivePower'\n",
    "df = df.withColumn(\"generation_indicator\", \n",
    "                   when(col(\"LV ActivePower\") < 200, \"Low\")\n",
    "                   .when((col(\"LV ActivePower\") >= 200) & (col(\"LV ActivePower\") < 600), \"Medium\")\n",
    "                   .when((col(\"LV ActivePower\") >= 600) & (col(\"LV ActivePower\") < 1000), \"High\")\n",
    "                   .otherwise(\"Exceptional\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

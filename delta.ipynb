{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, to_date, to_timestamp, current_date, current_timestamp, date_format\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, DateType, FloatType\n",
    "\n",
    "# SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"deltatable\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for the data\n",
    "schema = StructType([\n",
    "    StructField(\"Date/Time\", DateType(), True),\n",
    "    StructField(\"LV_ActivePower\", FloatType(), True),\n",
    "    StructField(\"Wind_Speed\", FloatType(), True),\n",
    "    StructField(\"Theoretical_Power_Curve\", FloatType(), True),\n",
    "    StructField(\"Wind_Direction\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Define Kafka source\n",
    "df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"sparkread\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Check if DataFrame is streaming\n",
    "if df.isStreaming:\n",
    "    print(\"\\nDataFrame is streaming. Monitoring for new data...\\n\")\n",
    "else:\n",
    "    print(\"\\nDataFrame is not streaming. No new data to monitor.\\n\")\n",
    "\n",
    "# Parse JSON data and select relevant columns\n",
    "json_df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "            .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "            .select(\"data.*\")\n",
    "\n",
    "# Apply transformations\n",
    "update_df = json_df.withColumn(\"signal_date\", to_date(col(\"Date/Time\"), \"yyyy-MM-dd\")) \\\n",
    "                   .withColumn(\"signal_tc\", to_timestamp(col(\"Date/Time\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "                   .withColumn(\"create_date\", date_format(current_date(), 'dd-MM-yyyy')) \\\n",
    "                   .withColumn(\"create_ts\", date_format(current_timestamp(), 'dd-MM-yyyy HH:mm:ss'))\n",
    "\n",
    "# Print schema of the updated DataFrame\n",
    "update_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import create_map, lit\n",
    "\n",
    "# Define the signal names\n",
    "signal_names = ['LV_ActivePower', 'Wind_Speed', 'Theoretical_Power_Curve', 'Wind_Direction']\n",
    "\n",
    "# Create a map column with signal names and their corresponding values\n",
    "map_expr = create_map(*[lit(signal).alias(signal) for signal in signal_names])\n",
    "\n",
    "# Add the map column to the DataFrame\n",
    "update_df = update_df.withColumn(\"signal\", map_expr)\n",
    "\n",
    "# Print schema of the updated DataFrame\n",
    "update_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import create_map, lit\n",
    "\n",
    "# Define the signal names and corresponding column names\n",
    "signal_names = ['LV ActivePower', 'Wind Speed', 'Theo_Power_Curve', 'Wind Direction']\n",
    "column_names = ['LV_ActivePower', 'Wind_Speed', 'Theoretical_Power_Curve', 'Wind_Direction']\n",
    "\n",
    "# Create a map column with signal names as keys and corresponding column names as values\n",
    "map_expr = create_map([lit(signal).alias(signal) for signal in signal_names],\n",
    "                      [lit(column).alias(column) for column in column_names])\n",
    "\n",
    "# Add the map column to the DataFrame\n",
    "update_df = update_df.withColumn(\"signal\", map_expr)\n",
    "\n",
    "# Print schema of the updated DataFrame\n",
    "update_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    {\n",
    "        \"sig_name\":\"LV ActivePower\",\n",
    "        \"sig_mapping_name\":\"LV ActivePower_average\"\n",
    "    },\n",
    "    {\n",
    "        \"sig_name\":\"Wind Speed\",\n",
    "        \"sig_mapping_name\":\"Wind Speed_average\"\n",
    "    },\n",
    "    {\n",
    "        \"sig_name\":\"Theoretical_Power_Curve\",\n",
    "        \"sig_mapping_name\":\"Theoretical_Power_Curve_average\"\n",
    "    },\n",
    "    {\n",
    "        \"sig_name\":\"Wind Direction\",\n",
    "        \"sig_mapping_name\":\"Wind Direction_average\"\n",
    "    },\n",
    "    {\n",
    "        \"sig_name\":\"Wind Direction\",\n",
    "        \"sig_mapping_name\":\"Wind Direction_average\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreateDataFrameFromJSON\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# JSON data\n",
    "json_data = [\n",
    "    {\"sig_name\": \"LV ActivePower\", \"sig_mapping_name\": \"LV ActivePower_average\"},\n",
    "    {\"sig_name\": \"Wind Speed\", \"sig_mapping_name\": \"Wind Speed_average\"},\n",
    "    {\"sig_name\": \"Theoretical_Power_Curve\", \"sig_mapping_name\": \"Theoretical_Power_Curve_average\"},\n",
    "    {\"sig_name\": \"Wind Direction\", \"sig_mapping_name\": \"Wind Direction_average\"},\n",
    "    {\"sig_name\": \"Wind Direction\", \"sig_mapping_name\": \"Wind Direction_average\"}\n",
    "]\n",
    "\n",
    "# Create DataFrame from JSON data\n",
    "df = spark.createDataFrame([Row(**x) for x in json_data])\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ChangeSignalNames\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# JSON data for signal mapping\n",
    "json_data = [\n",
    "    {\"sig_name\": \"LV ActivePower\", \"sig_mapping_name\": \"LV ActivePower_average\"},\n",
    "    {\"sig_name\": \"Wind Speed\", \"sig_mapping_name\": \"Wind Speed_average\"},\n",
    "    {\"sig_name\": \"Theoretical_Power_Curve\", \"sig_mapping_name\": \"Theoretical_Power_Curve_average\"},\n",
    "    {\"sig_name\": \"Wind Direction\", \"sig_mapping_name\": \"Wind Direction_average\"},\n",
    "    {\"sig_name\": \"Wind Direction\", \"sig_mapping_name\": \"Wind Direction_average\"}\n",
    "]\n",
    "\n",
    "# Create DataFrame from JSON data\n",
    "mapping_df = spark.createDataFrame([Row(**x) for x in json_data])\n",
    "\n",
    "# Perform broadcast join\n",
    "broadcast_df = df.join(broadcast(mapping_df),\n",
    "                       df.generation_indicator == mapping_df.sig_name,\n",
    "                       \"left_outer\")\n",
    "\n",
    "# Update the 'generation_indicator' column\n",
    "broadcast_df = broadcast_df.withColumn(\"generation_indicator\", broadcast_df.sig_mapping_name)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "broadcast_df = broadcast_df.drop(\"sig_name\", \"sig_mapping_name\")\n",
    "\n",
    "# Show the updated DataFrame\n",
    "broadcast_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exmaple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import random\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FakeDataFrame\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Fake data\n",
    "fake_data = [\n",
    "    {\"name\": \"Alice\", \"id\": 1, \"salary\": 50000},\n",
    "    {\"name\": \"Bob\", \"id\": 2, \"salary\": 60000},\n",
    "    {\"name\": \"Charlie\", \"id\": 3, \"salary\": 55000},\n",
    "    {\"name\": \"David\", \"id\": 4, \"salary\": 70000},\n",
    "    {\"name\": \"Eve\", \"id\": 5, \"salary\": 65000}\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(fake_data)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter DataFrame to select rows where 'id' is greater than 3\n",
    "filtered_df = df.filter(df['id'] > 3)\n",
    "\n",
    "# Select the 'id' column from the filtered DataFrame\n",
    "filtered_ids = filtered_df.select('id')\n",
    "\n",
    "# Show the values of the 'id' column\n",
    "filtered_ids.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrameSQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a DataFrame with some data\n",
    "data = [(\"Alice\", 1, 50000),\n",
    "        (\"Bob\", 2, 60000),\n",
    "        (\"Charlie\", 3, 55000),\n",
    "        (\"David\", 4, 70000),\n",
    "        (\"Eve\", 5, 65000)]\n",
    "columns = [\"name\", \"id\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"employee\")\n",
    "\n",
    "# Perform SQL operations\n",
    "sql_result = spark.sql(\"SELECT * FROM employee WHERE id > 3\")\n",
    "\n",
    "# Show the result\n",
    "sql_result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrameSQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a DataFrame with some data\n",
    "data = [(\"Alice\", 1, 50000),\n",
    "        (\"Bob\", 2, 60000),\n",
    "        (\"Charlie\", 3, 55000),\n",
    "        (\"David\", 4, 70000),\n",
    "        (\"Eve\", 5, 65000)]\n",
    "columns = [\"name\", \"id\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"employee\")\n",
    "\n",
    "# Perform SQL operation to select rows where 'id' and 'salary' are greater than 2 and 55000 respectively\n",
    "sql_result = spark.sql(\"SELECT * FROM employee WHERE id > 2 AND salary > 55000\")\n",
    "\n",
    "# Show the result\n",
    "sql_result.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
